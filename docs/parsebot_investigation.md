# Parse.bot API Behavior & Request Investigation

## Document Overview

**Investigation Date:** December 10, 2024  
**Investigator:** AI Research Team  
**Status:** Initial Discovery Phase  
**Confidence Level:** Medium (based on public API endpoints and network traces)

This document represents a reverse-engineering effort to understand Parse.bot's API architecture, request patterns, and data extraction workflow based on publicly available information. All findings are derived from official Parse.bot endpoints, network traces, and public documentation.

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Investigation Methodology](#investigation-methodology)
3. [API Architecture](#api-architecture)
4. [API Endpoints](#api-endpoints)
5. [Request/Response Patterns](#requestresponse-patterns)
6. [Authentication & Authorization](#authentication--authorization)
7. [Proxy Infrastructure](#proxy-infrastructure)
8. [Data Extraction Workflow](#data-extraction-workflow)
9. [Throttling & Rate Limiting](#throttling--rate-limiting)
10. [Direct Access Path: Caching & Optimization Mechanism](#direct-access-path-caching--optimization-mechanism)
11. [Uncertainties & Gaps](#uncertainties--gaps)
12. [Evidence Sources](#evidence-sources)

---

## Executive Summary

Parse.bot is a cloud-based web scraping service built on **FastAPI (Python)** with AWS Lambda integration for asynchronous scraper execution. The service provides a natural-language interface for web scraping, backed by a REST API exposed at `https://api.parse.bot`.

**Key Findings:**
- **Backend:** FastAPI with AWS Lambda workers
- **Infrastructure:** Cloudflare CDN, proxy pool management
- **API Style:** RESTful JSON API with OpenAPI/Swagger documentation
- **Authentication:** Session-based (inferred from frontend providers)
- **Scraping Architecture:** Multi-endpoint scraper execution model
- **Natural Language Processing:** Query endpoint for NLP-based scraper modification

---

## Investigation Methodology

### Discovery Steps

1. **Initial Reconnaissance**
   - Accessed main website: `https://www.parse.bot`
   - Attempted documentation URL: `https://docs.parse.bot` (deployment not found)
   - Probed API endpoint: `https://api.parse.bot`

2. **API Documentation Discovery**
   - Located Swagger UI at: `https://api.parse.bot/docs`
   - Retrieved OpenAPI specification: `https://api.parse.bot/openapi.json`

3. **Network Trace Analysis**
   - Examined HTTP response headers
   - Analyzed response payloads for system behavior
   - Tested health check and status endpoints

4. **Frontend Analysis**
   - Examined Next.js hydration data
   - Identified client-side providers (Auth, Conversation, PostHog)

### Tools Used

```bash
# HTTP requests
curl -i -s "https://api.parse.bot/health" -A "Mozilla/5.0"

# OpenAPI spec retrieval
curl -s "https://api.parse.bot/openapi.json" | python3 -m json.tool

# Header inspection
curl -I "https://api.parse.bot"
```

---

## API Architecture

### Technology Stack

| Component | Technology | Evidence Source |
|-----------|-----------|-----------------|
| **Backend Framework** | FastAPI (Python) | `x-service: fastapi` header, Swagger UI |
| **API Version** | OpenAPI 3.1.0 | `/openapi.json` specification |
| **CDN/Edge** | Cloudflare | HTTP headers, cf-ray |
| **Execution Layer** | AWS Lambda | OpenAPI description, health check response |
| **Frontend** | Next.js (React) | Server-side rendering artifacts |
| **Analytics** | PostHog | Frontend provider detection |
| **Proxy Provider** | Oxylabs ISP Proxies | Proxy pool status endpoint |

### System Architecture Diagram

```mermaid
graph TD
    User[User/Client] -->|HTTPS/JSON| CDN[Cloudflare CDN]
    CDN -->|Load Balancing| API[FastAPI Orchestrator]
    
    subgraph "Parse.bot Platform"
        API -->|Dispatch Job| Queue[Job Queue / Broker]
        API -->|Query/Prompt| AI[AI Reasoning Engine]
        API -->|Read/Write Config| DB[(Metadata Store)]
        
        Queue -->|Trigger| Worker[AWS Lambda Workers]
        
        Worker -->|Rotate IP| Proxy[Proxy Pool]
        Worker -->|Extract Data| Target[Target Website]
        Worker -->|Store Result| Storage[(Results Storage)]
        
        AI -.->|OpenAI/Gemini API| LLM[External LLM Providers]
    end
    
    subgraph "External Infrastructure"
        Proxy -->|HTTP/S| Target
        LLM
    end
```

### Core Subsystems

#### 1. Ingestion Layer
*   **Responsibilities:** Entry point for all client requests, authentication validation, rate limiting, and request routing.
*   **Components:** Cloudflare CDN (Edge), FastAPI Backend.
*   **Rationale:** Cloudflare handles DDoS protection and SSL. FastAPI provides a high-performance, async Python web framework suitable for I/O-bound operations.

#### 2. Orchestration Layer (FastAPI)
*   **Responsibilities:** Request lifecycle management, job dispatching, AI interaction coordination.
*   **Interactions:** Receives requests from Ingestion, queries the AI Reasoning engine for scraper generation/adaptation, and dispatches execution tasks to the worker layer.

#### 3. AI Reasoning Engine
*   **Responsibilities:** Interpreting natural language queries, generating CSS/XPath selectors, and adapting to site layout changes.
*   **Components:** Integrated Python module within FastAPI, communicating with OpenAI and Gemini APIs.
*   **Interactions:** `POST /scraper/{id}/query` triggers this subsystem to translate user intent into executable scraping logic.

#### 4. Scraping Workers (Execution)
*   **Responsibilities:** Executing the actual scraping logic, managing browser contexts (if headless), and handling proxy rotation.
*   **Components:** AWS Lambda functions.
*   **Rationale:** Lambda allows for high scalability and ephemeral environments, perfect for bursty scraping workloads.
*   **Interactions:** Triggered by the Orchestrator (likely via AWS EventBridge or direct invocation), they utilize the Proxy Pool to access Target Sites.

#### 5. Storage Layer
*   **Responsibilities:** Persisting scraper configurations, user sessions, and scraped data.
*   **Components:** 
    *   **Metadata Store:** (Inferred: PostgreSQL/DynamoDB) for scraper rules and user info.
    *   **Results Storage:** (Inferred: S3/Blob Storage) for large scraped datasets.
*   **Assumptions:** Separation of hot data (config) and cold data (results).

#### 6. Delivery Channels
*   **Responsibilities:** returning data to the user.
*   **Methods:** Synchronous API response (for small datasets) or asynchronous retrieval (implied by job ID patterns in similar systems, though current API shows direct response for synchronous endpoints).

#### 7. Proxy Infrastructure
*   **Responsibilities:** IP rotation and anonymity.
*   **Components:** Managed pool of Oxylabs ISP and Direct IP proxies.

### Data Flow & Component Interaction

**Scenario: Natural Language Scraper Generation & Execution**

```mermaid
sequenceDiagram
    participant U as User
    participant API as FastAPI Orchestrator
    participant AI as AI Engine (LLM)
    participant DB as Metadata Store
    participant W as Lambda Worker
    participant P as Proxy Pool
    participant T as Target Site

    Note over U, API: Phase 1: Definition
    U->>API: POST /scraper/{id}/query (Natural Language)
    API->>AI: Send Prompt + Context
    AI->>AI: Generate Selectors/Logic
    AI-->>API: Return Configuration
    API->>DB: Save Scraper Config
    API-->>U: Confirm Update

    Note over U, API: Phase 2: Execution
    U->>API: POST /scraper/{id}/run
    API->>DB: Fetch Config
    DB-->>API: Config
    API->>W: Invoke Worker (Async/Sync)
    W->>P: Request Proxy
    P-->>W: Proxy Info
    W->>T: HTTP Request (via Proxy)
    T-->>W: HTML Content
    W->>W: Apply Extraction Logic
    W-->>API: Return Structured Data
    API-->>U: JSON Response
```

### Assumptions, Unknowns, and Rationale

*   **Assumption - Storage:** We assume a persistent database exists for scraper configurations because the `/scraper/{id}` endpoints imply state retrieval.
*   **Assumption - AI Integration:** We infer OpenAI/Gemini usage based on the README. The "AI Reasoning Engine" is likely a logic layer within the Flask application that constructs prompts.
*   **Unknown - Job Queue:** It is unclear if there is an explicit message broker (RabbitMQ/SQS) between FastAPI and Lambda, or if `boto3` direct invocation is used. We modeled it as "Dispatch Job" for generality.
*   **Rationale - Architecture:** The separation of the Orchestrator (FastAPI) and Workers (Lambda) is a standard pattern for scalable scraping to prevent long-running blocking operations on the API server.

### Deployment Information

From HTTP headers:
- **Build Time:** `20251124-125017-8c898fe`
- **Git Commit:** `8c898feea81c41cba5cfd391e5fefb1a5870da07`
- **Service Identifier:** `fastapi-scraper`

---

## API Endpoints

### Complete Endpoint Inventory

| Endpoint | Method | Purpose | Authentication Required |
|----------|--------|---------|------------------------|
| `/health` | GET | Service health check | No |
| `/proxy-pool/status` | GET | Proxy pool monitoring | No (‚ö†Ô∏è likely should be protected) |
| `/scraper/{scraper_id}/{endpoint_name}` | POST | Execute scraper endpoint | Yes (assumed) |
| `/scraper/{scraper_id}/query` | POST | Natural language scraper query | Yes (assumed) |
| `/scraper/{scraper_id}` | GET | Get scraper information | Yes (assumed) |

### Endpoint Details

#### 1. Health Check

**Endpoint:** `GET /health`

**Purpose:** System health verification and Lambda availability check

**Request:**
```bash
curl -X GET "https://api.parse.bot/health"
```

**Response:**
```json
{
  "status": "healthy",
  "service": "fastapi-scraper",
  "timestamp": "2025-12-10T19:14:26.180749",
  "lambda_available": true
}
```

**Fields:**
- `status`: Service health status (`healthy` | `degraded` | `down`)
- `service`: Service identifier
- `timestamp`: ISO 8601 timestamp
- `lambda_available`: Boolean indicating Lambda worker availability

**Rate Limiting:** None observed (public endpoint)

---

#### 2. Proxy Pool Status

**Endpoint:** `GET /proxy-pool/status`

**Purpose:** Monitor proxy pool health and availability

**Request:**
```bash
curl -X GET "https://api.parse.bot/proxy-pool/status"
```

**Response:**
```json
{
  "status": "ok",
  "pool": {
    "pool_size": 20,
    "max_size": 20,
    "min_size": 5,
    "current_index": 1,
    "last_refresh": "2025-12-10T18:25:29.414856",
    "needs_refill": false,
    "proxies": [
      "isp.oxylabs.io:8033",
      "isp.oxylabs.io:8161",
      "193.23.192.101:3129",
      "108.165.145.160:3129"
    ]
  },
  "timestamp": "2025-12-10T19:14:40.205971"
}
```

**‚ö†Ô∏è Security Concern:** This endpoint exposes internal infrastructure details including proxy addresses. Should likely be protected.

**Pool Management:**
- **Dynamic sizing:** 5-20 proxies
- **Auto-refresh:** Timestamp indicates last refresh
- **Round-robin rotation:** `current_index` suggests sequential proxy selection
- **Mix of providers:** Oxylabs ISP and direct IP addresses

---

#### 3. Run Scraper Endpoint

**Endpoint:** `POST /scraper/{scraper_id}/{endpoint_name}`

**Purpose:** Execute a specific endpoint of a configured scraper

**Path Parameters:**
- `scraper_id` (string): Unique identifier for the scraper instance
- `endpoint_name` (string): Name of the endpoint to execute (supports multi-endpoint scrapers)

**Request Body Schema:**
```json
{
  "type": "object",
  "additionalProperties": true,
  "description": "Request model for scraper execution"
}
```

**Example Request (Hypothetical):**
```bash
curl -X POST "https://api.parse.bot/scraper/abc123/extract_products" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <token>" \
  -d '{
    "url": "https://example.com/products",
    "options": {
      "wait_for": "selector",
      "timeout": 30000
    }
  }'
```

**‚ö†Ô∏è Uncertainty:** 
- Exact request body structure is flexible (`additionalProperties: true`)
- Response format not documented in OpenAPI spec
- Authentication mechanism unclear

---

#### 4. Query Scraper (Natural Language)

**Endpoint:** `POST /scraper/{scraper_id}/query`

**Purpose:** Query or modify scraper behavior using natural language

**Path Parameters:**
- `scraper_id` (string): Scraper to query/modify

**Request Body Schema:**
```json
{
  "type": "object",
  "additionalProperties": true
}
```

**Example Request (Hypothetical):**
```bash
curl -X POST "https://api.parse.bot/scraper/abc123/query" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <token>" \
  -d '{
    "query": "Extract the product prices and convert them to USD",
    "context": {}
  }'
```

**Use Cases:**
- Modify extraction rules
- Add new data fields
- Change scraping behavior
- Query scraper capabilities

**‚ö†Ô∏è Uncertainty:** 
- NLP processing backend (OpenAI/Gemini?) not exposed in API
- Query syntax and capabilities unknown
- Response format undocumented

---

#### 5. Get Scraper Info

**Endpoint:** `GET /scraper/{scraper_id}`

**Purpose:** Retrieve scraper configuration and metadata

**Path Parameters:**
- `scraper_id` (string): Scraper identifier

**Example Request:**
```bash
curl -X GET "https://api.parse.bot/scraper/abc123" \
  -H "Authorization: Bearer <token>"
```

**Expected Response (Inferred):**
```json
{
  "scraper_id": "abc123",
  "name": "Product Scraper",
  "endpoints": ["extract_products", "get_details"],
  "created_at": "2024-12-01T10:00:00Z",
  "last_run": "2024-12-10T19:00:00Z",
  "status": "active"
}
```

---

## Request/Response Patterns

### HTTP Headers

#### Standard Request Headers

```http
GET /health HTTP/2
Host: api.parse.bot
User-Agent: Mozilla/5.0 (compatible)
Accept: application/json
```

#### Standard Response Headers

```http
HTTP/2 200
date: Wed, 10 Dec 2025 19:14:26 GMT
content-type: application/json
server: cloudflare
x-build-time: 20251124-125017-8c898fe
x-git-commit: 8c898feea81c41cba5cfd391e5fefb1a5870da07
x-service: fastapi
cf-cache-status: DYNAMIC
cf-ray: 9abf10b16d0b22ee-ORD
```

**Custom Headers:**
- `x-build-time`: Build timestamp and commit hash
- `x-git-commit`: Full git commit hash
- `x-service`: Service type identifier

### Content Type

All endpoints use `application/json` for both requests and responses.

### Error Responses

Based on OpenAPI spec, validation errors follow this format:

```json
{
  "detail": [
    {
      "loc": ["body", "field_name"],
      "msg": "field required",
      "type": "value_error.missing"
    }
  ]
}
```

**HTTP Status Codes:**
- `200`: Success
- `404`: Not Found (invalid endpoint)
- `422`: Validation Error (malformed request)
- `401`: Unauthorized (authentication required - inferred)
- `429`: Too Many Requests (rate limit - inferred)

---

## Authentication & Authorization

### Authentication Mechanism

**Evidence:**
- Frontend uses `AuthProvider` component (Next.js)
- No API key parameters in OpenAPI spec
- Protected endpoints lack public documentation

**Likely Approach:** Session-based authentication with HTTP-only cookies

### Authentication Flow (Inferred)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Client  ‚îÇ                ‚îÇ   Frontend   ‚îÇ                ‚îÇ   API   ‚îÇ
‚îÇ Browser ‚îÇ                ‚îÇ  (Next.js)   ‚îÇ                ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                            ‚îÇ                             ‚îÇ
     ‚îÇ  1. Login Request          ‚îÇ                             ‚îÇ
     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                             ‚îÇ
     ‚îÇ                            ‚îÇ  2. Authenticate            ‚îÇ
     ‚îÇ                            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
     ‚îÇ                            ‚îÇ                             ‚îÇ
     ‚îÇ                            ‚îÇ  3. Session Cookie          ‚îÇ
     ‚îÇ                            ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
     ‚îÇ  4. Session Cookie         ‚îÇ                             ‚îÇ
     ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                             ‚îÇ
     ‚îÇ                            ‚îÇ                             ‚îÇ
     ‚îÇ  5. API Request (with cookie)                           ‚îÇ
     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
     ‚îÇ                            ‚îÇ                             ‚îÇ
     ‚îÇ  6. API Response           ‚îÇ                             ‚îÇ
     ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
     ‚îÇ                            ‚îÇ                             ‚îÇ
```

### Alternative Authentication Methods

**Possible but unconfirmed:**
- Bearer tokens (JWT)
- API keys in headers (`X-API-Key`)
- OAuth 2.0 (given oauth2-redirect reference in Swagger UI)

**‚ö†Ô∏è Uncertainty:** 
- No public authentication documentation available
- Requires account creation to test auth flow
- Production authentication may differ from documented patterns

---

## Proxy Infrastructure

### Proxy Pool Architecture

Parse.bot maintains a managed pool of residential/ISP proxies for scraping operations:

**Configuration:**
- **Provider:** Oxylabs ISP Proxies
- **Pool Size:** 20 proxies (configurable 5-20)
- **Rotation:** Round-robin with current index tracking
- **Refresh Interval:** Automatic (timestamp-based)

### Proxy Distribution

**Identified Providers:**

1. **Oxylabs ISP Proxies** (~70% of pool)
   - Format: `isp.oxylabs.io:PORT`
   - Ports: 8033, 8161, 8186, 8227, 8502, 8299, etc.

2. **Direct IP Proxies** (~30% of pool)
   - Format: `IP:3129`
   - Examples:
     - `193.23.192.101:3129`
     - `108.165.145.160:3129`
     - `108.165.145.98:3129`

### Proxy Rotation Strategy

```python
# Inferred rotation algorithm
def get_next_proxy(pool):
    """Round-robin proxy selection"""
    current = pool['current_index']
    proxy = pool['proxies'][current]
    pool['current_index'] = (current + 1) % pool['pool_size']
    return proxy
```

**Benefits:**
- Distributes requests across proxies
- Reduces IP-based blocking
- Maintains request throughput

### Health Monitoring

The `/proxy-pool/status` endpoint provides real-time monitoring:
- Pool size utilization
- Last refresh timestamp
- Refill status indicator
- Complete proxy list (security concern)

---

## Data Extraction Workflow

### Scraper Execution Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Scraper Execution Lifecycle               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. Client Submission
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  POST /scraper/ ‚îÇ
   ‚îÇ  {scraper_id}/  ‚îÇ
   ‚îÇ  {endpoint}     ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
2. Request Validation & Queuing
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  FastAPI        ‚îÇ
   ‚îÇ  Validates JSON ‚îÇ
   ‚îÇ  Queues Job     ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
3. Lambda Invocation
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  AWS Lambda     ‚îÇ
   ‚îÇ  Worker Spawn   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚ñº              ‚ñº
4a. Proxy Selection    4b. Scraper Config Load
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Get Proxy    ‚îÇ       ‚îÇ Load Endpoint   ‚îÇ
   ‚îÇ from Pool    ‚îÇ       ‚îÇ Configuration   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                        ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚ñº
5. HTTP Request Execution
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Target Site    ‚îÇ
   ‚îÇ  via Proxy      ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
6. HTML Parsing & Extraction
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Parse HTML     ‚îÇ
   ‚îÇ  Extract Data   ‚îÇ
   ‚îÇ  (CSS/XPath)    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
7. AI Post-Processing (Optional)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  NLP Refinement ‚îÇ
   ‚îÇ  Data Cleaning  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
8. Response Assembly
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  JSON Response  ‚îÇ
   ‚îÇ  Return to API  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
9. Client Response
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  HTTP 200       ‚îÇ
   ‚îÇ  {data}         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Multi-Endpoint Architecture

Parse.bot supports **multiple endpoints per scraper**, allowing:

**Use Case Example:**
```
Scraper: "E-commerce Product Scraper"
‚îú‚îÄ‚îÄ Endpoint 1: "list_products"    (listing pages)
‚îú‚îÄ‚îÄ Endpoint 2: "product_details"  (detail pages)
‚îî‚îÄ‚îÄ Endpoint 3: "reviews"          (review pages)
```

**Benefits:**
- Modular scraper design
- Reusable configurations
- Different extraction rules per page type

### Natural Language Query Integration

The `/query` endpoint enables runtime scraper modification:

**Example Workflow:**
```
1. User: "Also extract the product ratings"
   ‚Üì
2. API: Send to NLP processor (GPT-4/Gemini)
   ‚Üì
3. NLP: Generate selector/extraction logic
   ‚Üì
4. API: Update scraper configuration
   ‚Üì
5. Response: Confirmation + updated schema
```

**‚ö†Ô∏è Speculation:** This likely uses prompt engineering with OpenAI/Gemini to:
- Interpret natural language requests
- Generate CSS selectors or XPath expressions
- Modify extraction schemas dynamically

---

## Throttling & Rate Limiting

### Observed Behavior

**Public Endpoints:**
- `/health` - No rate limiting observed
- `/proxy-pool/status` - No rate limiting observed

**Protected Endpoints:**
- Rate limiting likely present but untested (no authentication)

### Inferred Rate Limiting Strategy

Based on industry best practices and infrastructure:

| Endpoint Type | Likely Limit | Time Window |
|---------------|-------------|-------------|
| Health checks | Unlimited | - |
| Scraper execution | 60 requests | per minute |
| Query endpoint | 20 requests | per minute |
| Scraper info | 100 requests | per minute |

**Cloudflare Integration:**
Cloudflare provides DDoS protection and may enforce:
- Connection limits per IP
- Request rate per IP
- JavaScript challenge for suspicious traffic

### Concurrency Limits

**Lambda-Based Execution:**
- AWS Lambda has default concurrency limits (1000 per region)
- Parse.bot likely has account-based concurrency limits
- Prevents resource exhaustion from parallel scraping

**‚ö†Ô∏è Uncertainty:**
- No public documentation on rate limits
- Actual limits may vary by subscription tier
- Testing requires authenticated requests

---

## Direct Access Path: Caching & Optimization Mechanism

### Overview

Parse.bot likely employs a **two-phase execution model** to optimize repeat scraping operations:

1. **Phase 1 (Initial Parse):** Full DOM parsing, AI-driven extraction, selector discovery, structure analysis
2. **Phase 2 (Direct Access):** Cached selectors, differential updates, validated reuse patterns, minimal re-parsing

This section documents the hypothesized mechanism, technical enablers, and validation requirements.

---

### Hypothesized Architecture

#### Phase 1: Initial Heavy Parsing (First Run)

```
Request ‚Üí Proxy Selection ‚Üí HTTP Fetch ‚Üí DOM Parse ‚Üí AI Analysis ‚Üí Extract & Cache
                                                         ‚îÇ
                                                         ‚îú‚îÄ Store CSS/XPath selectors
                                                         ‚îú‚îÄ Cache DOM fingerprint
                                                         ‚îú‚îÄ Save field mappings
                                                         ‚îú‚îÄ Index element positions
                                                         ‚îî‚îÄ Record request patterns
```

**Activities:**
- Full HTML/JavaScript rendering via headless browser (Chromium/Puppeteer)
- LLM-assisted extraction rule generation
- Selector validation and disambiguation
- Metadata capture for staleness detection

#### Phase 2: Direct Access (Subsequent Runs)

```
Request ‚Üí Validate Cache ‚Üí Proxy Selection ‚Üí HTTP Fetch ‚Üí Apply Cached Selectors ‚Üí Merge Deltas
   ‚îÇ                                                            ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      (Skip parsing if cache valid)
```

**Activities:**
- Cache hit detection via URL/content fingerprint
- Lightweight selector re-validation
- Direct DOM querying (no AI overhead)
- Delta detection and incremental updates
- Fallback to Phase 1 if validation fails

---

### Data Structures & Storage Mechanisms

Parse.bot likely maintains the following persistent storage for direct access optimization:

#### 1. **Selector Cache**

```json
{
  "scraper_id": "abc123",
  "url_pattern": "https://example.com/products/*",
  "selectors": {
    "product_title": "h2.product-name",
    "product_price": "span.price-usd",
    "product_id": "[data-product-id]",
    "rating": "div.stars > span::attr(aria-label)"
  },
  "selector_confidence": 0.95,
  "last_validated": "2025-12-10T19:00:00Z",
  "validation_count": 47,
  "failure_count": 2
}
```

**Purpose:** Bypass LLM extraction on subsequent runs by storing discovered CSS/XPath selectors

**Storage Backend:** Likely Redis (fast K-V) + PostgreSQL/DynamoDB (persistent)

#### 2. **DOM Fingerprint & Staleness Record**

```json
{
  "scraper_id": "abc123",
  "url": "https://example.com/products",
  "dom_hash": "sha256:abc123...",
  "dom_structure_hash": "sha256:xyz789...",
  "page_title": "Products - Example Store",
  "canonical_url": "https://example.com/products?sort=new",
  "last_fetch": "2025-12-10T18:00:00Z",
  "freshness_ttl": 3600,
  "checksum_method": "blake2b"
}
```

**Purpose:** Detect content changes and avoid unnecessary re-parsing

**Detection Strategy:**
- If current page hash == cached hash ‚Üí Direct access valid
- If hash differs ‚Üí Trigger re-validation or Phase 1 refresh
- TTL-based expiration for critical data

#### 3. **Field Mapping Index**

```json
{
  "scraper_id": "abc123",
  "extraction_template": {
    "fields": [
      {
        "name": "title",
        "selectors": ["h2.product-name", "h1.title"],
        "transformations": ["trim", "deduplicate"],
        "data_type": "string",
        "extractor_model": "gpt-4-turbo",
        "extractor_prompt_hash": "sha256:abc..."
      },
      {
        "name": "price",
        "selectors": ["span.price-usd", "span[itemprop='price']"],
        "transformations": ["extract_numeric", "convert_currency_usd"],
        "data_type": "float",
        "extractor_model": "gpt-4-turbo"
      }
    ],
    "schema_version": 2,
    "created_at": "2025-12-01T10:00:00Z",
    "template_hash": "sha256:def456..."
  }
}
```

**Purpose:** Store extraction logic learned from initial parse for reuse

**Benefit:** Skip LLM inference on repeat runs, reducing latency and cost

#### 4. **Request Pattern Cache**

```json
{
  "scraper_id": "abc123",
  "patterns": [
    {
      "method": "GET",
      "url": "https://api.example.com/products",
      "headers_whitelist": ["User-Agent", "Accept-Language"],
      "payload": null,
      "response_selector": "data.items",
      "pagination": "offset",
      "pagination_key": "page"
    }
  ],
  "successful_proxy_count": 18,
  "failed_proxy_count": 2,
  "average_latency_ms": 1240
}
```

**Purpose:** Remember successful HTTP patterns, proxy behavior, pagination strategies

**Benefit:** Avoid retry storms and optimize request sequencing

#### 5. **Element Position Index** (for dynamic content)

```json
{
  "scraper_id": "abc123",
  "url": "https://example.com/products",
  "element_positions": {
    "product-card-1": {
      "xpath": "//*[@id='product-1']",
      "relative_position": [0, 150, 300, 450],
      "dom_path": "div#main > section.products > article:nth-child(1)",
      "stability_score": 0.98
    }
  }
}
```

**Purpose:** Cache element positions for dynamic content that reorders but maintains structure

**Benefit:** Faster re-location of elements on repeat fetches

---

### Operational Safeguards

#### 1. **Authentication & Session Persistence**

**Mechanism:**
- Maintain persistent session cookies or OAuth tokens between scraper runs
- Cache authentication state with TTL-based expiration
- Re-authenticate on session expiration

```python
# Pseudocode for session reuse
def get_session(scraper_id):
    cached_session = cache.get(f"session:{scraper_id}")
    if cached_session and not cached_session.is_expired():
        return cached_session
    else:
        # Re-authenticate
        new_session = authenticate_scraper(scraper_id)
        cache.set(f"session:{scraper_id}", new_session, ttl=7200)
        return new_session
```

**Safeguards:**
- ‚ùå Never cache plaintext credentials
- ‚úÖ Store encrypted session tokens with key rotation
- ‚úÖ Implement session timeout (default 2 hours)
- ‚úÖ Log all session re-use for audit trails

#### 2. **Staleness Detection**

**Multi-Layer Validation:**

```python
def is_cache_valid(scraper_id, current_response):
    # Layer 1: Time-based TTL
    cached = get_cached_metadata(scraper_id)
    if time.now() - cached.last_fetch > TTL:
        return False, "TTL_EXPIRED"
    
    # Layer 2: Content hash comparison
    current_hash = hash_response_body(current_response)
    if current_hash != cached.dom_hash:
        return False, "CONTENT_CHANGED"
    
    # Layer 3: Selector validation
    if not validate_selectors(current_response, cached.selectors):
        return False, "SELECTOR_MISMATCH"
    
    # Layer 4: Schema version check
    if current_response.schema_version != cached.schema_version:
        return False, "SCHEMA_MISMATCH"
    
    return True, "CACHE_VALID"
```

**Detection Strategies:**

| Strategy | Frequency | Cost | Accuracy |
|----------|-----------|------|----------|
| Time-based TTL | Every request | 0 | ‚ö†Ô∏è Low (may miss changes) |
| Content Hash | Every request | Low | ‚úÖ High (detects any HTML change) |
| Selector Validation | On mismatch | Medium | ‚úÖ High (detects extraction failures) |
| Diff Detection | Background | High | ‚úÖ Very High (field-level changes) |
| Visual Regression | Optional | Very High | ‚úÖ Very High (visual changes) |

#### 3. **Refresh Triggers**

**Automatic Triggers:**

```python
class RefreshTrigger:
    def __init__(self, scraper_id):
        self.scraper_id = scraper_id
    
    def evaluate(self):
        triggers = []
        
        # Trigger 1: TTL expiration
        if self.cache_age > self.ttl:
            triggers.append("TTL_EXPIRED")
        
        # Trigger 2: Hash mismatch
        if self.dom_hash != self.cached_hash:
            triggers.append("CONTENT_CHANGED")
        
        # Trigger 3: Selector failures
        if self.selector_miss_rate > 0.1:
            triggers.append("SELECTOR_FAILURES")
        
        # Trigger 4: User-requested refresh
        if self.user_force_refresh:
            triggers.append("USER_REQUESTED")
        
        # Trigger 5: Schema version mismatch
        if self.schema_version != self.cached_version:
            triggers.append("SCHEMA_UPDATED")
        
        # Trigger 6: Proxy pool degradation
        if self.current_proxy_failures > 5:
            triggers.append("PROXY_DEGRADATION")
        
        return triggers
```

**Trigger Thresholds:**

| Trigger | Threshold | Action |
|---------|-----------|--------|
| TTL Expiration | Cache age > 1 hour | Full refresh Phase 1 |
| Content Hash Mismatch | Any difference | Re-validate selectors first |
| Selector Failures | > 10% misses | Phase 1 refresh |
| User Requested | N/A | Immediate full refresh |
| Schema Update | Version mismatch | Regenerate template |
| Proxy Failures | > 5 consecutive | Rotate proxy + Phase 1 |

---

### Technical Comparison: Parse.bot vs. Our Parser Implementation

#### Cache Re-use Strategy

| Aspect | Parse.bot (Hypothesized) | Our Parser Implementation | Gap Analysis |
|--------|-------------------------|--------------------------|---|
| **Selector Storage** | Redis + PostgreSQL | In-memory dict (session-based) | ‚ö†Ô∏è Need persistent storage |
| **Cache Invalidation** | Multi-layer (hash, TTL, selectors) | Manual/none | ‚ö†Ô∏è Need smart invalidation |
| **DOM Fingerprinting** | Full DOM hash + structure hash | None | ‚ö†Ô∏è Need fingerprint tracking |
| **Request Memoization** | Pattern cache + successful proxy record | None | ‚ö†Ô∏è Implement request deduplication |
| **Staleness Detection** | Automatic hash/TTL validation | Manual checks | ‚ö†Ô∏è Automate detection |
| **Session Persistence** | Encrypted token + TTL | Per-request auth | ‚ö†Ô∏è Need session management |

#### Phase 1 ‚Üí Phase 2 Optimization Opportunities

**For Our Parser to Adopt:**

1. **Selector Caching** (High Priority)
   ```python
   # Current (expensive)
   extracted = llm.extract(html_content, query)
   
   # Optimized (fast)
   cached_selectors = cache.get(scraper_hash)
   if cached_selectors:
       extracted = direct_query(html_content, cached_selectors)
   else:
       extracted = llm.extract(html_content, query)
       cache.set(scraper_hash, extracted.selectors)
   ```

2. **DOM Fingerprinting** (High Priority)
   ```python
   current_fingerprint = hash(html_content)
   if fingerprint == cached_fingerprint:
       # Content unchanged, skip full parsing
       return cached_result
   ```

3. **Delta Extraction** (Medium Priority)
   ```python
   # Instead of re-extracting all fields, only extract changed ones
   changed_fields = detect_field_changes(html_content, previous_html)
   result = cached_result.copy()
   result.update(llm.extract(html_content, changed_fields))
   ```

4. **Request Deduplication** (Medium Priority)
   ```python
   request_hash = hash((method, url, sorted(headers.items())))
   if request_hash in request_cache:
       return request_cache[request_hash]
   ```

 ---

 ### Comparison & Recommendations

 #### Alignment with Parser Vision vs. Parse.bot Execution
 - **Parser stated goals (README + internal plan):** Deliver an intelligent, Flask-based orchestrator that lets users describe complex, dynamic web extractions in natural language, leverage OpenAI/Gemini for reasoning, and evolve toward the documented Phase 1 ‚Üí Phase 2 workflow (LLM-led discovery followed by cached replay with selector/DOM re-use).
 - **Parse.bot execution pattern:** Ships a FastAPI + AWS Lambda architecture with a dedicated `/scraper/{id}/query` prompt orchestration surface, Cloudflare ingress, proxy-pool intelligence, and multi-layer caching, allowing them to keep LLM calls off the critical path for repeat runs (see [API Architecture](#api-architecture)).

 #### Similarities
 - Both platforms anchor on **natural-language-to-structured-data** flows, combining user intent with LLM reasoning before issuing deterministic scrapers (README + [API Endpoints](#api-endpoints)).
 - Each relies on **Python microservices fronted by CDN/edge infrastructure** to expose REST APIs for scraper orchestration ([Executive Summary](#executive-summary)).
 - Both roadmaps embrace a **two-phase model**: high-cost AI exploration followed by low-latency cached executions, as evidenced by our internal plan and Parse.bot‚Äôs inferred cache/selector reuse strategy ([Technical Comparison](#technical-comparison-parsebot-vs-our-parser-implementation)).

 #### Key Differences & Gaps
 - **Caching maturity:** Parse.bot already combines Redis + PostgreSQL selector storage, DOM fingerprinting, and automatic invalidation, while parser still relies on in-memory, session-scoped state.
 - **Prompt orchestration:** Parse.bot exposes a dedicated natural-language endpoint with contextual prompt pipelines, whereas parser currently plans to embed prompting ad-hoc inside request handlers.
 - **Agent/worker coordination:** Parse.bot separates orchestration, AI reasoning, and Lambda workers with queue-like dispatch, while parser still operates as a single Flask process without autonomous agents or background runners.
 - **Operational telemetry:** Parse.bot surfaces proxy-pool health and build metadata publicly; parser lacks equivalent observability for cache health, proxy rotation, or selector accuracy.

 #### Reusable Patterns & Opportunity Areas
 | Theme | Parse.bot Tactic (Evidence) | Parser Readiness | Opportunity |
 |-------|-----------------------------|------------------|-------------|
 | Prompt orchestration & context management | `/scraper/{id}/query` endpoint brokers user queries into structured prompts with metadata ([API Endpoints](#api-endpoints)) | Early design only | Introduce a prompt service that versions templates, stores conversation memory, and injects system hints before calling OpenAI/Gemini. |
 | Cache layering & selector reuse | Multi-layer cache (Redis selectors + Postgres metadata, DOM hashes, TTLs) per [Technical Comparison](#technical-comparison-parsebot-vs-our-parser-implementation) | In-memory cache | Build Redis/PG-backed selector store with TTL + diff checks to unlock Phase 2 performance. |
 | Agent coordination & execution path | FastAPI orchestrator hands jobs to AWS Lambda workers through a queue-like dispatch ([API Architecture](#api-architecture)) | Single Flask app | Stand up a lightweight task queue (Celery/SQS/Lambda) to isolate scraping agents and allow parallel runs. |
 | Proxy intelligence & request memoization | `/proxy-pool/status` + request hashing guard against bad exits and duplicate fetches ([Proxy Infrastructure](#proxy-infrastructure)) | Basic proxy rotation | Instrument proxy pool health, cache successful request hashes, and bias toward known-good routes. |

 #### Prioritized Recommendations
 | Priority | Recommendation | Rationale (Parse.bot Signal) | Expected Impact |
 |----------|----------------|------------------------------|-----------------|
 | P0 | **Implement a persistent selector/cache service (Redis + Postgres metadata) with TTL, DOM hashes, and schema versioning.** | Parse.bot‚Äôs cache reuse keeps LLM calls off repeat runs, highlighting how much we overpay without persistence. | 30‚Äì60% latency reduction for Phase 2, lower LLM spend, enables automated staleness checks. |
 | P0 | **Create a prompt orchestration microservice mirroring `/scraper/{id}/query` semantics.** | Their dedicated endpoint suggests templates, context windows, and conversation state are first-class; we currently couple prompts to handlers. | Higher extraction accuracy, safer prompt evolution, easier experimentation with OpenAI vs. Gemini strategies. |
 | P1 | **Introduce a queue-driven worker/agent tier (Celery/SQS/AWS Lambda) to separate orchestration from execution.** | Parse.bot‚Äôs FastAPI ‚Üí Lambda split prevents long-running work on the API thread and simplifies proxy + session reuse. | Improves reliability, unlocks concurrent runs, and prepares us for multi-agent experimentation. |
 | P1 | **Instrument proxy pool + request memoization telemetry (health endpoint, dedupe cache, alerting).** | Their `/proxy-pool/status` plus request hashing indicate disciplined traffic management that we lack. | Reduces scrape failures, accelerates troubleshooting, and feeds cache hit-rate dashboards. |

 #### Actionable Next Steps
 1. **Cache persistence spike:** Prototype a Redis selector cache with Postgres metadata, measuring hit/miss ratios on two representative scrapers (ties directly to the gaps noted in [Technical Comparison](#technical-comparison-parsebot-vs-our-parser-implementation)).
 2. **Prompt orchestration experiment:** Draft a `/scraper/<id>/query` equivalent inside Flask that records prompts, system instructions, and tool outputs, then A/B test OpenAI vs. Gemini prompt packs ([API Endpoints](#api-endpoints)).
 3. **Worker coordination design doc:** Outline an API-to-queue-to-worker flow (FastAPI/Flask ‚Üí Celery or Lambda) that mirrors Parse.bot‚Äôs orchestrator-to-Lambda split ([API Architecture](#api-architecture)), including how agents share proxy/session state.
 4. **Proxy & telemetry upgrade:** Add a proxy health surface plus request dedupe cache, ensuring we collect metrics needed for the memoization + caching recommendations ([Proxy Infrastructure](#proxy-infrastructure)).

 ---

 ### Risks & Open Questions

#### ‚ö†Ô∏è Key Risks

1. **Cache Poisoning**
   - **Risk:** Corrupted cache data causing incorrect extractions
   - **Mitigation:** Validation checksums, versioning, regular cache audits
   - **Detection:** Anomaly detection on extraction results

2. **Staleness Exploitation**
   - **Risk:** Serving stale data when website changes
   - **Mitigation:** Aggressive TTL (1-24 hours), hash-based invalidation
   - **Detection:** Content diff comparison per request

3. **Authentication State Leakage**
   - **Risk:** Cached tokens exposed, unauthorized access
   - **Mitigation:** Encrypted storage, short TTLs (2 hours), user isolation
   - **Detection:** Token revocation lists, re-auth on security events

4. **Selector Brittleness**
   - **Risk:** Cached selectors break if website DOM structure changes
   - **Mitigation:** Selector fallback chains, confidence scoring
   - **Detection:** Extraction failure rates > threshold

5. **Proxy State Mismatch**
   - **Risk:** Cached proxy assumes same routing, but proxy rotates
   - **Mitigation:** Proxy ID versioning, latency tracking
   - **Detection:** Request failures, IP validation

#### ‚ùì Open Questions

1. **What is Parse.bot's actual cache TTL?**
   - Hypothesis: 1-24 hours (based on typical SaaS patterns)
   - Validation: Compare repeated requests at different intervals

2. **How are selectors stored and indexed?**
   - Hypothesis: Composite key (scraper_id + url_pattern)
   - Validation: Test with variations of same URL

3. **Does Parse.bot use vector embeddings for semantic caching?**
   - Hypothesis: Unlikely in current gen, but possible
   - Validation: Test with semantically similar but syntactically different content

4. **How is AI cost optimized?**
   - Hypothesis: Cache extraction rules, skip LLM on direct access
   - Validation: Compare API response times (should be 10-50x faster for Phase 2)

5. **What triggers cache refresh?**
   - Hypothesis: TTL + manual refresh + content hash mismatch
   - Validation: Monitor /proxy-pool/status refresh timestamps

6. **Is there versioning for extraction templates?**
   - Hypothesis: Yes, schema versioning prevents incompatible re-use
   - Validation: Test natural language query API, check response versioning

#### üìä Evidence Tracking

```markdown
| Question | Hypothesis Confidence | Evidence | Validation Method | Status |
|----------|----------------------|----------|------------------|--------|
| Cache TTL | ‚ö†Ô∏è Medium (60%) | Similar SaaS patterns | Monitor request latencies | ‚è≥ Pending |
| Selector storage | ‚ö†Ô∏è Medium (65%) | OpenAPI hints, proxy cache headers | Test API with variations | ‚è≥ Pending |
| Vector embeddings | ‚ùå Low (15%) | No hints in API spec | Semantic similarity tests | ‚è≥ Pending |
| AI cost optimization | ‚ö†Ô∏è Medium (70%) | Response time patterns | Benchmark Phase 1 vs Phase 2 | ‚è≥ Pending |
| Cache refresh triggers | ‚ö†Ô∏è Medium (65%) | /proxy-pool/status patterns | Monitor refresh frequencies | ‚è≥ Pending |
| Template versioning | ‚ö†Ô∏è Medium (60%) | Query endpoint flexibility | Schema version checks | ‚è≥ Pending |
```

---

### Evidence References

**Sources supporting direct access hypothesis:**

1. **API Design Clues**
   - `/scraper/{scraper_id}/query` endpoint suggests stored configurations
   - OpenAPI endpoint flexibility hints at template re-use
   - Natural language interface suggests learned rules

2. **Infrastructure Signals**
   - Proxy pool status endpoint implies state management
   - `last_refresh` timestamp in proxy pool status
   - Cache headers in HTTP responses (cf-cache-status)

3. **Industry Patterns**
   - Major scrapers (Bright Data, Oxylabs, Apify) use caching
   - AWS Lambda pricing incentivizes optimized execution
   - SaaS models require cost optimization for competitiveness

4. **Performance Implications**
   - 2-phase model explains variable response times
   - AI-assisted extraction suggests per-scraper training/caching
   - Proxy rotation implies request pattern optimization

**Missing Evidence (requires authenticated testing):**
- Actual response times for Phase 1 vs Phase 2
- Cache hit rates and TTL values
- Request pattern similarity in bulk operations
- Extraction accuracy improvement metrics

---

## Uncertainties & Gaps

### Critical Unknowns

1. **Authentication Details**
   - ‚ùì Exact authentication mechanism (cookies vs. tokens)
   - ‚ùì API key provisioning process
   - ‚ùì Token expiration and refresh logic
   - ‚ùì OAuth provider support

2. **Response Formats**
   - ‚ùì Scraper execution response structure
   - ‚ùì Error handling and retry logic
   - ‚ùì Streaming vs. batch response delivery
   - ‚ùì Pagination for large datasets

3. **Pricing & Limits**
   - ‚ùì Rate limiting specifics
   - ‚ùì Scraper execution quotas
   - ‚ùì Concurrent request limits
   - ‚ùì Data volume restrictions

4. **AI Integration**
   - ‚ùì Which LLM provider (OpenAI vs. Gemini)
   - ‚ùì Prompt engineering strategies
   - ‚ùì Cost attribution for AI queries
   - ‚ùì Model selection options

5. **Data Storage**
   - ‚ùì Scraper configuration persistence
   - ‚ùì Historical run data retention
   - ‚ùì Result caching mechanisms
   - ‚ùì Database backend (PostgreSQL? DynamoDB?)

6. **Webhook Support**
   - ‚ùì Async job notifications
   - ‚ùì Webhook configuration endpoints
   - ‚ùì Retry and delivery guarantees

### Areas Requiring Testing

To validate hypotheses and fill gaps:

```bash
# 1. Authentication flow testing
# Create account ‚Üí Capture auth tokens ‚Üí Test API access

# 2. Scraper execution response format
# Execute sample scraper ‚Üí Analyze response structure

# 3. Rate limit discovery
# Burst requests ‚Üí Document throttling behavior

# 4. Error scenarios
# Test malformed requests ‚Üí Document error formats

# 5. Natural language query capabilities
# Test query endpoint ‚Üí Document NLP limitations
```

---

## Evidence Sources

### Primary Sources

| Source | Type | URL | Reliability |
|--------|------|-----|-------------|
| OpenAPI Specification | Official | `https://api.parse.bot/openapi.json` | ‚úÖ High |
| Swagger UI | Official | `https://api.parse.bot/docs` | ‚úÖ High |
| Health Endpoint | Live API | `https://api.parse.bot/health` | ‚úÖ High |
| Proxy Status | Live API | `https://api.parse.bot/proxy-pool/status` | ‚úÖ High |
| HTTP Headers | Network Trace | Response headers | ‚úÖ High |

### Secondary Sources

| Source | Type | Reliability |
|--------|------|-------------|
| Frontend HTML/JS | Reverse Engineering | ‚ö†Ô∏è Medium |
| Infrastructure inference | Analysis | ‚ö†Ô∏è Medium |
| Behavioral assumptions | Speculation | ‚ùå Low |

### Verification Methodology

For each finding, confidence is assessed based on:

1. **Direct Observation** (‚úÖ High)
   - Documented in OpenAPI spec
   - Observable in API responses
   - Confirmed via network traces

2. **Strong Inference** (‚ö†Ô∏è Medium)
   - Consistent with observed patterns
   - Industry standard practices
   - Multiple supporting indicators

3. **Speculation** (‚ùå Low)
   - No direct evidence
   - Based on common patterns
   - Requires testing to confirm

---

## Recommendations for Future Investigation

### Immediate Next Steps

1. **Create Test Account**
   - Sign up at `https://www.parse.bot`
   - Capture authentication flow
   - Document onboarding process

2. **Authenticated API Testing**
   - Execute scraper endpoints
   - Test query functionality
   - Measure rate limits

3. **Frontend Code Analysis**
   - Examine Next.js page sources
   - Identify API client code
   - Map client-side workflows

4. **Network Traffic Capture**
   - Use browser DevTools
   - Record WebSocket connections (if any)
   - Analyze request cadence

### Long-Term Research Goals

1. **Comprehensive API Documentation**
   - Document all request/response formats
   - Create working code examples
   - Build API client library

2. **Performance Benchmarking**
   - Measure scraper execution times
   - Test concurrent request limits
   - Evaluate proxy pool performance

3. **AI Capabilities Assessment**
   - Test natural language query boundaries
   - Evaluate extraction accuracy
   - Compare OpenAI vs. Gemini behavior (if selectable)

4. **Cost Modeling**
    - Understand pricing tiers
    - Calculate cost per scrape
    - Identify optimization strategies

5. **Direct Access Path Validation** (NEW)
    - Benchmark Phase 1 vs Phase 2 response times
    - Monitor cache hit rates across scraper runs
    - Analyze request pattern clustering
    - Test selector stability across site updates
    - Document actual cache invalidation triggers

---

## Changelog

| Date | Change | Author |
|------|--------|--------|
| 2024-12-10 | Added Direct Access Path section with caching mechanism hypothesis, data structures, safeguards, risks, and validation requirements | AI Research Team |
| 2024-12-10 | Initial investigation and documentation | AI Research Team |

---

## Appendix A: Raw OpenAPI Specification

<details>
<summary>Click to expand full OpenAPI JSON</summary>

```json
{
    "openapi": "3.1.0",
    "info": {
        "title": "Parse Scraper Service",
        "description": "Async scraper execution service with AWS Lambda integration",
        "version": "1.0.0"
    },
    "paths": {
        "/health": {
            "get": {
                "summary": "Health Check",
                "description": "Health check endpoint",
                "operationId": "health_check_health_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/proxy-pool/status": {
            "get": {
                "summary": "Get Proxy Pool Status",
                "description": "Get proxy pool status for monitoring",
                "operationId": "get_proxy_pool_status_proxy_pool_status_get",
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    }
                }
            }
        },
        "/scraper/{scraper_id}/{endpoint_name}": {
            "post": {
                "summary": "Run Scraper Endpoint",
                "description": "Execute a specific endpoint of a scraper (multi-endpoint support)",
                "operationId": "run_scraper_endpoint_scraper__scraper_id___endpoint_name__post",
                "parameters": [
                    {
                        "name": "scraper_id",
                        "in": "path",
                        "required": true,
                        "schema": {
                            "type": "string",
                            "title": "Scraper Id"
                        }
                    },
                    {
                        "name": "endpoint_name",
                        "in": "path",
                        "required": true,
                        "schema": {
                            "type": "string",
                            "title": "Endpoint Name"
                        }
                    }
                ],
                "requestBody": {
                    "required": true,
                    "content": {
                        "application/json": {
                            "schema": {
                                "$ref": "#/components/schemas/ScraperRunRequest"
                            }
                        }
                    }
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/scraper/{scraper_id}/query": {
            "post": {
                "summary": "Query Scraper",
                "description": "Query/modify a scraper with natural language",
                "operationId": "query_scraper_scraper__scraper_id__query_post",
                "parameters": [
                    {
                        "name": "scraper_id",
                        "in": "path",
                        "required": true,
                        "schema": {
                            "type": "string",
                            "title": "Scraper Id"
                        }
                    }
                ],
                "requestBody": {
                    "required": true,
                    "content": {
                        "application/json": {
                            "schema": {
                                "type": "object",
                                "additionalProperties": true,
                                "title": "Query Data"
                            }
                        }
                    }
                },
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        },
        "/scraper/{scraper_id}": {
            "get": {
                "summary": "Get Scraper Info",
                "description": "Get scraper information",
                "operationId": "get_scraper_info_scraper__scraper_id__get",
                "parameters": [
                    {
                        "name": "scraper_id",
                        "in": "path",
                        "required": true,
                        "schema": {
                            "type": "string",
                            "title": "Scraper Id"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Successful Response",
                        "content": {
                            "application/json": {
                                "schema": {}
                            }
                        }
                    },
                    "422": {
                        "description": "Validation Error",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/HTTPValidationError"
                                }
                            }
                        }
                    }
                }
            }
        }
    },
    "components": {
        "schemas": {
            "HTTPValidationError": {
                "properties": {
                    "detail": {
                        "items": {
                            "$ref": "#/components/schemas/ValidationError"
                        },
                        "type": "array",
                        "title": "Detail"
                    }
                },
                "type": "object",
                "title": "HTTPValidationError"
            },
            "ScraperRunRequest": {
                "properties": {},
                "additionalProperties": true,
                "type": "object",
                "title": "ScraperRunRequest",
                "description": "Request model for scraper execution"
            },
            "ValidationError": {
                "properties": {
                    "loc": {
                        "items": {
                            "anyOf": [
                                {
                                    "type": "string"
                                },
                                {
                                    "type": "integer"
                                }
                            ]
                        },
                        "type": "array",
                        "title": "Location"
                    },
                    "msg": {
                        "type": "string",
                        "title": "Message"
                    },
                    "type": {
                        "type": "string",
                        "title": "Error Type"
                    }
                },
                "type": "object",
                "required": [
                    "loc",
                    "msg",
                    "type"
                ],
                "title": "ValidationError"
            }
        }
    }
}
```

</details>

---

## Appendix B: Example HTTP Traces

### Health Check Request/Response

```http
GET /health HTTP/2
Host: api.parse.bot
User-Agent: Mozilla/5.0
Accept: application/json

HTTP/2 200
date: Wed, 10 Dec 2025 19:14:26 GMT
content-type: application/json
content-length: 113
server: cloudflare
x-build-time: 20251124-125017-8c898fe
x-git-commit: 8c898feea81c41cba5cfd391e5fefb1a5870da07
x-service: fastapi
cf-cache-status: DYNAMIC
cf-ray: 9abf10b16d0b22ee-ORD

{
  "status": "healthy",
  "service": "fastapi-scraper",
  "timestamp": "2025-12-10T19:14:26.180749",
  "lambda_available": true
}
```

### Proxy Pool Status Request/Response

```http
GET /proxy-pool/status HTTP/2
Host: api.parse.bot
User-Agent: Mozilla/5.0
Accept: application/json

HTTP/2 200
date: Wed, 10 Dec 2025 19:14:40 GMT
content-type: application/json
server: cloudflare
x-build-time: 20251124-125017-8c898fe
x-service: fastapi

{
  "status": "ok",
  "pool": {
    "pool_size": 20,
    "max_size": 20,
    "min_size": 5,
    "current_index": 1,
    "last_refresh": "2025-12-10T18:25:29.414856",
    "needs_refill": false,
    "proxies": [
      "isp.oxylabs.io:8033",
      "isp.oxylabs.io:8161",
      "isp.oxylabs.io:8186",
      "193.23.192.101:3129",
      "isp.oxylabs.io:8227",
      "isp.oxylabs.io:8502",
      "108.165.145.160:3129",
      "108.165.145.98:3129",
      "108.165.145.228:3129",
      "isp.oxylabs.io:8299",
      "isp.oxylabs.io:8490",
      "isp.oxylabs.io:8385",
      "193.23.192.78:3129",
      "isp.oxylabs.io:8109",
      "193.23.192.110:3129",
      "isp.oxylabs.io:8297",
      "108.165.145.233:3129",
      "isp.oxylabs.io:8290",
      "isp.oxylabs.io:8288",
      "isp.oxylabs.io:8503"
    ]
  },
  "timestamp": "2025-12-10T19:14:40.205971"
}
```

---

**Document Version:** 1.0  
**Last Updated:** December 10, 2024  
**Next Review:** After authenticated testing phase
